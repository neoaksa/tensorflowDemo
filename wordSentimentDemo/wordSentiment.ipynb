{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordSentiment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/neoaksa/tensorflowDemo/blob/master/wordSentimentDemo/wordSentiment.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Lv37VxPDAj1z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# authenticate google drive\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-c6HVLKlAlYE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1w9gmYSJRYjV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! pip install pydrive\n",
        "# these classes allow you to request the Google drive API\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "def downloadFile(inputfilename,outputfilename):\n",
        "    downloaded = drive.CreateFile({'id': inputfilename})\n",
        "    # assume the file is called file.csv and it's located at the root of your drive\n",
        "    downloaded.GetContentFile(outputfilename)\n",
        "    \n",
        "# traning file download\n",
        "trainingFile = downloadFile(\"1adyPElLZ118U1aKVEeqrsVNX4b-VoVDm\",\"training.1600000.processed.noemoticon.csv\")\n",
        "# test file download\n",
        "testingFile = downloadFile(\"1-6lzGSZ-IkIjYiUULuhpoADPe55aSWcR\",\"testdata.manual.2009.06.14.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aT7Zpcl4ES82",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Download the file we just uploaded.\n",
        "# #\n",
        "# # Replace the assignment below with your file ID\n",
        "# # to download a different file.\n",
        "# #\n",
        "# # use native google drive API\n",
        "\n",
        "# import io\n",
        "# from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "\n",
        "# def downloadFile(file_id):\n",
        "#     request = drive_service.files().get_media(fileId=file_id)\n",
        "#     downloaded = io.BytesIO()\n",
        "#     downloader = MediaIoBaseDownload(downloaded, request)\n",
        "#     done = False\n",
        "#     while done is False:\n",
        "#         # _ is a placeholder for a progress object that we ignore.\n",
        "#         # (Our file is small, so we skip reporting progress.)\n",
        "#         _, done = downloader.next_chunk()\n",
        "\n",
        "#     downloaded.seek(0)\n",
        "#     return downloaded\n",
        "    \n",
        "# # traning file download\n",
        "# trainingFile = downloadFile(\"1adyPElLZ118U1aKVEeqrsVNX4b-VoVDm\")\n",
        "# # test file download\n",
        "# testingFile = downloadFile(\"1-6lzGSZ-IkIjYiUULuhpoADPe55aSWcR\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gdk8PtU0AwIz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepare data\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def init_process(fin, fout):\n",
        "    outfile = open(fout, 'a')\n",
        "    with open(fin, buffering=200000, encoding='latin-1') as f:\n",
        "        try:\n",
        "            # replace special characters\n",
        "            for line in f:\n",
        "                line = line.replace('\"', '')\n",
        "                # the first column is label\n",
        "                initial_polarity = line.split(',')[0]\n",
        "                # 0=negative 4=positive\n",
        "                if initial_polarity == '0':\n",
        "                    initial_polarity = [1, 0]\n",
        "                elif initial_polarity == '4':\n",
        "                    initial_polarity = [0, 1]\n",
        "                # the last column is input\n",
        "                tweet = line.split(',')[-1]\n",
        "                outline = str(initial_polarity) + ':::' + tweet\n",
        "                outfile.write(outline)\n",
        "        except Exception as e:\n",
        "            print(str(e))\n",
        "    outfile.close()\n",
        "\n",
        "# save for training and testing dataset\n",
        "init_process('training.1600000.processed.noemoticon.csv', 'train_set.csv')\n",
        "init_process('testdata.manual.2009.06.14.csv', 'test_set.csv')\n",
        "# download and check the pre-processing file\n",
        "# from google.colab import files\n",
        "# files.download('test_set.csv')\n",
        "# files.download('train_set.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uSoqD4x7XtII",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# if lack of some componets please run this chuck\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MjUes5a2WWRH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# create lexicon\n",
        "def create_lexicon(fin):\n",
        "    lexicon = []\n",
        "    with open(fin, 'r', buffering=100000, encoding='latin-1') as f:\n",
        "        try:\n",
        "            counter = 1\n",
        "            content = ''\n",
        "            for line in f:\n",
        "                counter += 1\n",
        "                # randomly pick up line for sampling to lexicon\n",
        "                if (counter / 2500.0).is_integer():\n",
        "                    tweet = line.split(':::')[1]\n",
        "                    content += ' ' + tweet\n",
        "                    words = word_tokenize(content)\n",
        "                    words = [lemmatizer.lemmatize(i) for i in words]\n",
        "                    lexicon = list(set(lexicon + words))\n",
        "                    print(counter, len(lexicon))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(str(e))\n",
        "\n",
        "    with open('lexicon.pickle', 'wb') as f:\n",
        "        pickle.dump(lexicon, f)\n",
        "\n",
        "# create lexicon and save to pickle\n",
        "create_lexicon('train_set.csv')\n",
        "\n",
        "# convert dataset input to vector\n",
        "def convert_to_vec(fin, fout, lexicon_pickle):\n",
        "    # open lexicon\n",
        "    with open(lexicon_pickle, 'rb') as f:\n",
        "        lexicon = pickle.load(f)\n",
        "    outfile = open(fout, 'a')\n",
        "    with open(fin, buffering=20000, encoding='latin-1') as f:\n",
        "        counter = 0\n",
        "        for line in f:\n",
        "            counter += 1\n",
        "            label = line.split(':::')[0]\n",
        "            tweet = line.split(':::')[1]\n",
        "            # tokenize to array\n",
        "            current_words = word_tokenize(tweet.lower())\n",
        "            # lemmatize for each element\n",
        "            current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
        "            # one-hot coding\n",
        "            features = np.zeros(len(lexicon))\n",
        "            for word in current_words:\n",
        "                if word.lower() in lexicon:\n",
        "                    index_value = lexicon.index(word.lower())\n",
        "                    # OR DO +=1, test both\n",
        "                    features[index_value] += 1\n",
        "\n",
        "            features = list(features)\n",
        "            outline = str(features) + '::' + str(label) + '\\n'\n",
        "            outfile.write(outline)\n",
        "\n",
        "        print(counter)\n",
        "\n",
        "\n",
        "convert_to_vec('test_set.csv', 'processed-test-set.csv', 'lexicon.pickle')\n",
        "\n",
        "\n",
        "def shuffle_data(fin):\n",
        "    df = pd.read_csv(fin, error_bad_lines=False)\n",
        "    df = df.iloc[np.random.permutation(len(df))]\n",
        "    print(df.head())\n",
        "    df.to_csv('train_set_shuffled.csv', index=False)\n",
        "\n",
        "\n",
        "shuffle_data('train_set.csv')\n",
        "\n",
        "# split the csv into x, y dataset\n",
        "def create_test_data_pickle(fin):\n",
        "    feature_sets = []\n",
        "    labels = []\n",
        "    counter = 0\n",
        "    with open(fin, buffering=20000) as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                features = list(eval(line.split('::')[0]))\n",
        "                label = list(eval(line.split('::')[1]))\n",
        "\n",
        "                feature_sets.append(features)\n",
        "                labels.append(label)\n",
        "                counter += 1\n",
        "            except:\n",
        "                pass\n",
        "    print(counter)\n",
        "    feature_sets = np.array(feature_sets)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "create_test_data_pickle('processed-test-set.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lgeriJAYa8Qs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# the following step will take very long time, so we need to check if we use GPU\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P2AWXSxCYOh8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "this snippet is used for build ANN net work, train the model and test it\n",
        "before you run this snippet, you should run word2vec.py first which transfer\n",
        "training and testing data to one-hot code vector.\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "# setting ANN variables\n",
        "with open(\"lexicon.pickle\", 'rb') as f:\n",
        "    lexicon = pickle.load(f)\n",
        "input_size = len(lexicon)\n",
        "output_size = 2\n",
        "structure = [input_size,500,500,output_size]\n",
        "x = tf.placeholder(dtype='float',shape=[None,structure[0]])\n",
        "y = tf.placeholder(dtype='float')\n",
        "batch_size = 30\n",
        "epoch_max = 10\n",
        "total_batches = int(1600000 / batch_size)\n",
        "\n",
        "# feedforward Model\n",
        "def netural_network_model(x):\n",
        "    # l is output value from active function\n",
        "    l = 0\n",
        "    l_prev = 0\n",
        "    for i in range(1,len(structure)):\n",
        "        # create each layer structure\n",
        "        hidden_layer = {'weight':tf.Variable(tf.random_normal([structure[i-1],structure[i]])),\n",
        "                        'biases':tf.Variable(tf.random_normal([structure[i]]))}\n",
        "        # input layer--> first hidden layer\n",
        "        if i == 1:\n",
        "            l = tf.add(tf.matmul(x, hidden_layer['weight']),hidden_layer['biases'])\n",
        "            l = tf.nn.relu(l)\n",
        "            l_prev = l\n",
        "        # --->output layer without active function\n",
        "        elif i == len(structure)-1:\n",
        "            l = tf.add(tf.matmul(l_prev, hidden_layer['weight']), hidden_layer['biases'])\n",
        "        # hidden layer ---> hidden layer with Relu active function\n",
        "        else:\n",
        "            l = tf.add(tf.matmul(l_prev, hidden_layer['weight']), hidden_layer['biases'])\n",
        "            l = tf.nn.relu(l)\n",
        "            l_prev = l\n",
        "    return l\n",
        "\n",
        "\n",
        "tf_log = 'tf.log'\n",
        "\n",
        "\n",
        "def train_neural_network(x):\n",
        "    # prediction, cost and gradient decrease\n",
        "    prediction = netural_network_model(x)\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))\n",
        "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        saver = tf.train.Saver()\n",
        "        try:\n",
        "            epoch = int(open(tf_log, 'r').read().split('\\n')[-2]) + 1\n",
        "            print('STARTING:', epoch)\n",
        "        except:\n",
        "            epoch = 1\n",
        "\n",
        "        while epoch <= epoch_max:\n",
        "            # continue the session data\n",
        "            if epoch != 1:\n",
        "                saver.restore(sess, \"./model.ckpt\")\n",
        "            epoch_loss = 1\n",
        "            # load the prepared lexicon\n",
        "            with open('lexicon.pickle', 'rb') as f:\n",
        "                lexicon = pickle.load(f)\n",
        "            # load shuffled traning dataset\n",
        "            with open('train_set_shuffled.csv', buffering=20000, encoding='latin-1') as f:\n",
        "                batch_x = []\n",
        "                batch_y = []\n",
        "                batches_run = 0\n",
        "                for line in f:\n",
        "                    label = line.split(':::')[0]\n",
        "                    tweet = line.split(':::')[1]\n",
        "                    current_words = word_tokenize(tweet.lower())\n",
        "                    current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
        "\n",
        "                    features = np.zeros(len(lexicon))\n",
        "\n",
        "                    for word in current_words:\n",
        "                        if word.lower() in lexicon:\n",
        "                            index_value = lexicon.index(word.lower())\n",
        "                            # OR DO +=1, test both\n",
        "                            features[index_value] += 1\n",
        "                    line_x = list(features)\n",
        "                    line_y = eval(label)\n",
        "                    batch_x.append(line_x)\n",
        "                    batch_y.append(line_y)\n",
        "                    if len(batch_x) >= batch_size:\n",
        "                        _, c = sess.run([optimizer, cost], feed_dict={x: np.array(batch_x),\n",
        "                                                                      y: np.array(batch_y)})\n",
        "                        epoch_loss += c\n",
        "                        batch_x = []\n",
        "                        batch_y = []\n",
        "                        batches_run += 1\n",
        "                        # print('Batch run:', batches_run, '/', total_batches, '| Epoch:', epoch, '| Batch Loss:', c, )\n",
        "            # save session for each epoch\n",
        "            saver.save(sess, \"./model.ckpt\")\n",
        "            print('Epoch', epoch, 'completed out of', epoch_max, 'loss:', epoch_loss)\n",
        "            with open(tf_log, 'a') as f:\n",
        "                f.write(str(epoch) + '\\n')\n",
        "            epoch += 1\n",
        "\n",
        "\n",
        "train_neural_network(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dSsj1-E4JJAw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from functools import singledispatch\n",
        "\n",
        "@singledispatch\n",
        "def test_neural_network():\n",
        "    prediction = netural_network_model(x)\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(sess.run(tf.global_variables_initializer()))\n",
        "        saver = tf.train.Saver()\n",
        "        # load the traning session for model\n",
        "        try:\n",
        "            saver.restore(sess, \"model.ckpt\")\n",
        "        except Exception as e:\n",
        "            print(str(e))\n",
        "\n",
        "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "        # be ware there is no optimizer, since we dont need backprepergate\n",
        "        feature_sets = []\n",
        "        labels = []\n",
        "        counter = 0\n",
        "        # load test dataset\n",
        "        with open('processed-test-set.csv', buffering=20000) as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    features = list(eval(line.split('::')[0]))\n",
        "                    label = list(eval(line.split('::')[1]))\n",
        "                    feature_sets.append(features)\n",
        "                    labels.append(label)\n",
        "                    counter += 1\n",
        "                except:\n",
        "                    pass\n",
        "        print('Tested', counter, 'samples.')\n",
        "        test_x = np.array(feature_sets)\n",
        "        test_y = np.array(labels)\n",
        "        print('Accuracy:', accuracy.eval({x: test_x, y: test_y}))\n",
        "\n",
        "test_neural_network()\n",
        "\n",
        "@test_neural_network.register(str)\n",
        "def _(input_data):\n",
        "    prediction = netural_network_model(x)\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(sess.run(tf.global_variables_initializer()))\n",
        "        saver = tf.train.Saver()\n",
        "        with open('lexicon.pickle', 'rb') as f:\n",
        "            lexicon = pickle.load(f)\n",
        "        # load the session\n",
        "        try:\n",
        "            saver.restore(sess, \"model.ckpt\")\n",
        "        except Exception as e:\n",
        "            print(str(e))\n",
        "        # lemmatize the input data\n",
        "        current_words = word_tokenize(input_data.lower())\n",
        "        current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
        "        # one hot coding\n",
        "        features = np.zeros(len(lexicon))\n",
        "        for word in current_words:\n",
        "            if word.lower() in lexicon:\n",
        "                index_value = lexicon.index(word.lower())\n",
        "                # OR DO +=1, test both\n",
        "                features[index_value] += 1\n",
        "        features = np.array(list(features))\n",
        "        # pos: [1,0] , argmax: 0\n",
        "        # neg: [0,1] , argmax: 1\n",
        "        result = (sess.run(tf.argmax(prediction.eval(feed_dict={x: [features]}), 1)))\n",
        "        if result[0] == 0:\n",
        "            print('Positive:', input_data)\n",
        "        elif result[0] == 1:\n",
        "            print('Negative:', input_data)\n",
        "\n",
        "test_neural_network(\"I hate you\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qu7bI-xQ7tJi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}