{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordSentiment.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/neoaksa/tensorflowDemo/blob/master/wordSentimentDemo/wordSentiment.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Lv37VxPDAj1z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# authenticate google drive\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-c6HVLKlAlYE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aT7Zpcl4ES82",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "09fb581f-5b69-4f05-d34b-88e3072cf928"
      },
      "cell_type": "code",
      "source": [
        "# Download the file we just uploaded.\n",
        "#\n",
        "# Replace the assignment below with your file ID\n",
        "# to download a different file.\n",
        "#\n",
        "# A file ID looks like: 1uBtlaggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "\n",
        "def downloadFile(file_id):\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    downloaded = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(downloaded, request)\n",
        "    done = False\n",
        "    while done is False:\n",
        "        # _ is a placeholder for a progress object that we ignore.\n",
        "        # (Our file is small, so we skip reporting progress.)\n",
        "        _, done = downloader.next_chunk()\n",
        "\n",
        "    downloaded.seek(0)\n",
        "    print('Downloaded file contents are: {}'.format(downloaded.read()))\n",
        "    \n",
        "# traning file download\n",
        "downloadFile(\"1adyPElLZ118U1aKVEeqrsVNX4b-VoVDm\")\n",
        "# test file download\n",
        "downloadFile(\"-6lzGSZ-IkIjYiUULuhpoADPe55aSWcR\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "HttpError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e7ed814e9f2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdownloadFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1adyPElLZ118U1aKVEeqrsVNX4b-VoVDm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# test file download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdownloadFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-6lzGSZ-IkIjYiUULuhpoADPe55aSWcR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-e7ed814e9f2b>\u001b[0m in \u001b[0;36mdownloadFile\u001b[0;34m(file_id)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# _ is a placeholder for a progress object that we ignore.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# (Our file is small, so we skip reporting progress.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdownloaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mnext_chunk\u001b[0;34m(self, num_retries)\u001b[0m\n\u001b[1;32m    692\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mMediaDownloadProgress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_progress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHttpError\u001b[0m: <HttpError 404 when requesting https://www.googleapis.com/drive/v3/files/-6lzGSZ-IkIjYiUULuhpoADPe55aSWcR?alt=media returned \"File not found: -6lzGSZ-IkIjYiUULuhpoADPe55aSWcR.\">"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Gdk8PtU0AwIz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepare data\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def init_process(fin, fout):\n",
        "    outfile = open(fout, 'a')\n",
        "    with open(fin, buffering=200000, encoding='latin-1') as f:\n",
        "        try:\n",
        "            # replace special characters\n",
        "            for line in f:\n",
        "                line = line.replace('\"', '')\n",
        "                # the first column is label\n",
        "                initial_polarity = line.split(',')[0]\n",
        "                # 0=negative 4=positive\n",
        "                if initial_polarity == '0':\n",
        "                    initial_polarity = [1, 0]\n",
        "                elif initial_polarity == '4':\n",
        "                    initial_polarity = [0, 1]\n",
        "                # the last column is input\n",
        "                tweet = line.split(',')[-1]\n",
        "                outline = str(initial_polarity) + ':::' + tweet\n",
        "                outfile.write(outline)\n",
        "        except Exception as e:\n",
        "            print(str(e))\n",
        "    outfile.close()\n",
        "\n",
        "# save for training and testing dataset\n",
        "init_process('sentiment140/training.1600000.processed.noemoticon.csv', 'train_set.csv')\n",
        "init_process('sentiment140/testdata.manual.2009.06.14.csv', 'test_set.csv')\n",
        "\n",
        "# create lexicon\n",
        "def create_lexicon(fin):\n",
        "    lexicon = []\n",
        "    with open(fin, 'r', buffering=100000, encoding='latin-1') as f:\n",
        "        try:\n",
        "            counter = 1\n",
        "            content = ''\n",
        "            for line in f:\n",
        "                counter += 1\n",
        "                # randomly pick up line for sampling to lexicon\n",
        "                if (counter / 2500.0).is_integer():\n",
        "                    tweet = line.split(':::')[1]\n",
        "                    content += ' ' + tweet\n",
        "                    words = word_tokenize(content)\n",
        "                    words = [lemmatizer.lemmatize(i) for i in words]\n",
        "                    lexicon = list(set(lexicon + words))\n",
        "                    print(counter, len(lexicon))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(str(e))\n",
        "\n",
        "    with open('lexicon.pickle', 'wb') as f:\n",
        "        pickle.dump(lexicon, f)\n",
        "\n",
        "# create lexicon and save to pickle\n",
        "create_lexicon('train_set.csv')\n",
        "\n",
        "# convert dataset input to vector\n",
        "def convert_to_vec(fin, fout, lexicon_pickle):\n",
        "    # open lexicon\n",
        "    with open(lexicon_pickle, 'rb') as f:\n",
        "        lexicon = pickle.load(f)\n",
        "    outfile = open(fout, 'a')\n",
        "    with open(fin, buffering=20000, encoding='latin-1') as f:\n",
        "        counter = 0\n",
        "        for line in f:\n",
        "            counter += 1\n",
        "            label = line.split(':::')[0]\n",
        "            tweet = line.split(':::')[1]\n",
        "            # tokenize to array\n",
        "            current_words = word_tokenize(tweet.lower())\n",
        "            # lemmatize for each element\n",
        "            current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
        "            # one-hot coding\n",
        "            features = np.zeros(len(lexicon))\n",
        "            for word in current_words:\n",
        "                if word.lower() in lexicon:\n",
        "                    index_value = lexicon.index(word.lower())\n",
        "                    # OR DO +=1, test both\n",
        "                    features[index_value] += 1\n",
        "\n",
        "            features = list(features)\n",
        "            outline = str(features) + '::' + str(label) + '\\n'\n",
        "            outfile.write(outline)\n",
        "\n",
        "        print(counter)\n",
        "\n",
        "\n",
        "convert_to_vec('test_set.csv', 'processed-test-set.csv', 'lexicon.pickle')\n",
        "\n",
        "\n",
        "def shuffle_data(fin):\n",
        "    df = pd.read_csv(fin, error_bad_lines=False)\n",
        "    df = df.iloc[np.random.permutation(len(df))]\n",
        "    print(df.head())\n",
        "    df.to_csv('train_set_shuffled.csv', index=False)\n",
        "\n",
        "\n",
        "shuffle_data('train_set.csv')\n",
        "\n",
        "# split the csv into x, y dataset\n",
        "def create_test_data_pickle(fin):\n",
        "    feature_sets = []\n",
        "    labels = []\n",
        "    counter = 0\n",
        "    with open(fin, buffering=20000) as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                features = list(eval(line.split('::')[0]))\n",
        "                label = list(eval(line.split('::')[1]))\n",
        "\n",
        "                feature_sets.append(features)\n",
        "                labels.append(label)\n",
        "                counter += 1\n",
        "            except:\n",
        "                pass\n",
        "    print(counter)\n",
        "    feature_sets = np.array(feature_sets)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "create_test_data_pickle('processed-test-set.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}