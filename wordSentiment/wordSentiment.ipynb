{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordSentiment.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/neoaksa/tensorflowDemo/blob/master/wordSentiment/wordSentiment.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Lv37VxPDAj1z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# authenticate google drive\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-c6HVLKlAlYE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aT7Zpcl4ES82",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "5094df1f-e1cf-45f0-9d98-a412642a53c7"
      },
      "cell_type": "code",
      "source": [
        "# Download the file we just uploaded.\n",
        "#\n",
        "# Replace the assignment below with your file ID\n",
        "# to download a different file.\n",
        "#\n",
        "# A file ID looks like: 1uBtlaggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "\n",
        "def downloadFile(file_id):\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    downloaded = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(downloaded, request)\n",
        "    done = False\n",
        "    while done is False:\n",
        "        # _ is a placeholder for a progress object that we ignore.\n",
        "        # (Our file is small, so we skip reporting progress.)\n",
        "        _, done = downloader.next_chunk()\n",
        "\n",
        "    downloaded.seek(0)\n",
        "    return downloaded\n",
        "    \n",
        "# traning file download\n",
        "trainingFile = downloadFile(\"1adyPElLZ118U1aKVEeqrsVNX4b-VoVDm\")\n",
        "trainingFile.GetContentFile('training.1600000.processed.noemoticon.csv')\n",
        "# test file download\n",
        "testingFile = downloadFile(\"1-6lzGSZ-IkIjYiUULuhpoADPe55aSWcR\")\n",
        "testingFile.GetContentFile('testdata.manual.2009.06.14.csv')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-175c2dbcacb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# traning file download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtrainingFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownloadFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1adyPElLZ118U1aKVEeqrsVNX4b-VoVDm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrainingFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetContentFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training.1600000.processed.noemoticon.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# test file download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtestingFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownloadFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1-6lzGSZ-IkIjYiUULuhpoADPe55aSWcR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_io.BytesIO' object has no attribute 'GetContentFile'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Gdk8PtU0AwIz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepare data\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def init_process(fin, fout):\n",
        "    outfile = open(fout, 'a')\n",
        "    with open(fin, buffering=200000, encoding='latin-1') as f:\n",
        "        try:\n",
        "            # replace special characters\n",
        "            for line in f:\n",
        "                line = line.replace('\"', '')\n",
        "                # the first column is label\n",
        "                initial_polarity = line.split(',')[0]\n",
        "                # 0=negative 4=positive\n",
        "                if initial_polarity == '0':\n",
        "                    initial_polarity = [1, 0]\n",
        "                elif initial_polarity == '4':\n",
        "                    initial_polarity = [0, 1]\n",
        "                # the last column is input\n",
        "                tweet = line.split(',')[-1]\n",
        "                outline = str(initial_polarity) + ':::' + tweet\n",
        "                outfile.write(outline)\n",
        "        except Exception as e:\n",
        "            print(str(e))\n",
        "    outfile.close()\n",
        "\n",
        "# save for training and testing dataset\n",
        "init_process('sentiment140/training.1600000.processed.noemoticon.csv', 'train_set.csv')\n",
        "init_process('sentiment140/testdata.manual.2009.06.14.csv', 'test_set.csv')\n",
        "\n",
        "# create lexicon\n",
        "def create_lexicon(fin):\n",
        "    lexicon = []\n",
        "    with open(fin, 'r', buffering=100000, encoding='latin-1') as f:\n",
        "        try:\n",
        "            counter = 1\n",
        "            content = ''\n",
        "            for line in f:\n",
        "                counter += 1\n",
        "                # randomly pick up line for sampling to lexicon\n",
        "                if (counter / 2500.0).is_integer():\n",
        "                    tweet = line.split(':::')[1]\n",
        "                    content += ' ' + tweet\n",
        "                    words = word_tokenize(content)\n",
        "                    words = [lemmatizer.lemmatize(i) for i in words]\n",
        "                    lexicon = list(set(lexicon + words))\n",
        "                    print(counter, len(lexicon))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(str(e))\n",
        "\n",
        "    with open('lexicon.pickle', 'wb') as f:\n",
        "        pickle.dump(lexicon, f)\n",
        "\n",
        "# create lexicon and save to pickle\n",
        "create_lexicon('train_set.csv')\n",
        "\n",
        "# convert dataset input to vector\n",
        "def convert_to_vec(fin, fout, lexicon_pickle):\n",
        "    # open lexicon\n",
        "    with open(lexicon_pickle, 'rb') as f:\n",
        "        lexicon = pickle.load(f)\n",
        "    outfile = open(fout, 'a')\n",
        "    with open(fin, buffering=20000, encoding='latin-1') as f:\n",
        "        counter = 0\n",
        "        for line in f:\n",
        "            counter += 1\n",
        "            label = line.split(':::')[0]\n",
        "            tweet = line.split(':::')[1]\n",
        "            # tokenize to array\n",
        "            current_words = word_tokenize(tweet.lower())\n",
        "            # lemmatize for each element\n",
        "            current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
        "            # one-hot coding\n",
        "            features = np.zeros(len(lexicon))\n",
        "            for word in current_words:\n",
        "                if word.lower() in lexicon:\n",
        "                    index_value = lexicon.index(word.lower())\n",
        "                    # OR DO +=1, test both\n",
        "                    features[index_value] += 1\n",
        "\n",
        "            features = list(features)\n",
        "            outline = str(features) + '::' + str(label) + '\\n'\n",
        "            outfile.write(outline)\n",
        "\n",
        "        print(counter)\n",
        "\n",
        "\n",
        "convert_to_vec('test_set.csv', 'processed-test-set.csv', 'lexicon.pickle')\n",
        "\n",
        "\n",
        "def shuffle_data(fin):\n",
        "    df = pd.read_csv(fin, error_bad_lines=False)\n",
        "    df = df.iloc[np.random.permutation(len(df))]\n",
        "    print(df.head())\n",
        "    df.to_csv('train_set_shuffled.csv', index=False)\n",
        "\n",
        "\n",
        "shuffle_data('train_set.csv')\n",
        "\n",
        "# split the csv into x, y dataset\n",
        "def create_test_data_pickle(fin):\n",
        "    feature_sets = []\n",
        "    labels = []\n",
        "    counter = 0\n",
        "    with open(fin, buffering=20000) as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                features = list(eval(line.split('::')[0]))\n",
        "                label = list(eval(line.split('::')[1]))\n",
        "\n",
        "                feature_sets.append(features)\n",
        "                labels.append(label)\n",
        "                counter += 1\n",
        "            except:\n",
        "                pass\n",
        "    print(counter)\n",
        "    feature_sets = np.array(feature_sets)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "create_test_data_pickle('processed-test-set.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}